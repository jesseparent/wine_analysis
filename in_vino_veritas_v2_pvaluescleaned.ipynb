{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from csv file\n",
    "red_wine_df = pd.read_csv('resources/winequality-red.csv', sep=';')\n",
    "white_wine_df = pd.read_csv('resources/winequality-white.csv', sep=';')\n",
    "\n",
    "# Create a new column 'color' and assign '1' to all rows for red wine\n",
    "red_wine_df['color'] = 1\n",
    "\n",
    "# Create a new column 'color' and assign '0' to all rows for white wine\n",
    "white_wine_df['color'] = 0\n",
    "\n",
    "# Create a new dataframe 'wine_df' by combining red_wine_df and white_wine_df and reset the index\n",
    "wine_df = pd.concat([red_wine_df, white_wine_df], ignore_index=True)\n",
    "\n",
    "# Display wine_df\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the dataframe\n",
    "wine_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe's info\n",
    "wine_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for null values\n",
    "wine_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any null values\n",
    "wine_df = wine_df.dropna().reset_index(drop=True)\n",
    "\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "wine_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index\n",
    "wine_df = wine_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a count of the unique values in the quality column\n",
    "wine_df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned data to a new csv file\n",
    "wine_df.to_csv('resources/winequality-cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(wine_df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=wine_df, x='quality', hue='color')\n",
    "plt.title(\"Wine Quality Distribution by Type\")\n",
    "plt.xlabel(\"Quality\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Wine Color\", labels=[\"White\", \"Red\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the color column due to it not being relevant to quality, drop the free sulfur dioxide column due to it being highly correlated with total sulfur dioxide\n",
    "cleaned_wine = wine_df.drop(['color', 'free sulfur dioxide', 'chlorides', 'citric acid', 'fixed acidity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a value counts on the quality column\n",
    "cleaned_wine['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_plot = ['alcohol', 'volatile acidity', 'citric acid']\n",
    "for feature in features_to_plot:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='quality', y=feature, data=cleaned_wine)\n",
    "    plt.title(f\"{feature.title()} by Wine Quality\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['alcohol', 'volatile acidity', 'density', 'quality']\n",
    "sns.pairplot(cleaned_wine[top_features], hue='quality', palette='coolwarm')\n",
    "plt.suptitle(\"Pairplot of Selected Features\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='quality', y='alcohol', data=cleaned_wine)\n",
    "plt.title(\"Alcohol Distribution by Wine Quality\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Test and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for the quality column\n",
    "\n",
    "\n",
    "# Two Bins for 0-5 and 6-10\n",
    "bins = (0, 5, 10)\n",
    "\n",
    "# Name the bins 0 for low quality and 1 for high quality\n",
    "group_names = [0, 1]\n",
    "\n",
    "# Rename teh values in the quality column to the bin names\n",
    "cleaned_wine['quality'] = pd.cut(cleaned_wine['quality'], bins=bins, labels=group_names)\n",
    "\n",
    "# List unique values in the quality column\n",
    "cleaned_wine['quality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_wine['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = cleaned_wine.drop(columns= ['quality'])\n",
    "y= cleaned_wine['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = sm.OLS(y_train, X_train).fit()\n",
    "pvals = lr.pvalues.sort_values()\n",
    "for index, value in pvals.items():\n",
    "    print(f\"{index}: {value:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnl = sm.MNLogit(y, X).fit()\n",
    "pvals = mnl.pvalues\n",
    "pvals_sorted = pvals.iloc[:, 0].sort_values()\n",
    "for index, value in pvals_sorted.items():\n",
    "    print(f\"{index}: {value:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = sm.Logit(y_train, X_train).fit()\n",
    "pvals = logit.pvalues.sort_values()\n",
    "for index, value in pvals.items():\n",
    "    print(f\"{index}: {value:4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = y_train.value_counts()\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter   \n",
    "# Apply SMOTE on the scaled training data\n",
    "smote_model = SMOTE(random_state=42)\n",
    "X_resampled_smote, y_resampled_smote = smote_model.fit_resample(X_train, y_train)\n",
    "\n",
    "# Count distinct values for the resampled target data\n",
    "print(y_resampled_smote.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "# Apply SMOTEENN on the scaled training data\n",
    "smoteenn_model = SMOTEENN(random_state=42)\n",
    "X_resampled_smoteenn, y_resampled_smoteenn = smoteenn_model.fit_resample(X_train, y_train)\n",
    "# Count distinct values for the resampled target data\n",
    "print(y_resampled_smoteenn.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC RUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# calculate  a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "#print the results of the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AUC ROC curve tools\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "# calculate the ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='orange', label='ROC curve (area = %0.2f)' % roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new RandomForestClassier model\n",
    "rf_model = RandomForestClassifier()\n",
    "# Fit the resampled data the new model\n",
    "rf_model.fit(X_train, y_train)\n",
    "# Predict labels for resampled testing features\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Print classification reports\n",
    "print(f\"Classification Report - Original Data\")\n",
    "print(classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new RandomForestClassier model with SMOTE data\n",
    "rf_smote_model = RandomForestClassifier()\n",
    "# Fit the resampled data the new model\n",
    "rf_smote_model.fit(X_resampled_smote, y_resampled_smote)\n",
    "# Predict labels for resampled testing features\n",
    "rf_smote_y_pred = rf_smote_model.predict(X_test)\n",
    "\n",
    "# Print classification reports\n",
    "print(f\"Classification Report - SMOTE Data\")\n",
    "print(classification_report(y_test, rf_smote_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new RandomForestClassier model with SMOTEENN data\n",
    "rf_smoteenn_model = RandomForestClassifier()\n",
    "# Fit the resampled data the new model\n",
    "rf_smoteenn_model.fit(X_resampled_smoteenn, y_resampled_smoteenn)\n",
    "# Predict labels for resampled testing features\n",
    "rf_smoteenn_y_pred = rf_smoteenn_model.predict(X_test)\n",
    "\n",
    "# Print classification reports\n",
    "print(f\"Classification Report - SMOTE Data\")\n",
    "print(classification_report(y_test, rf_smoteenn_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification reports\n",
    "print(f\"Classification Report - Original Data\")\n",
    "print(classification_report(y_test, rf_y_pred))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Resampled Data - SMOTE\")\n",
    "print(classification_report(y_test, rf_smote_y_pred))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Resampled Data - SMOTEENN\")\n",
    "print(classification_report(y_test, rf_smoteenn_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the class distribution before SMOTE\n",
    "print(\"Original training class distribution:\", Counter(y_train))\n",
    "\n",
    "# Print the class distribution after SMOTE\n",
    "print(\"Resampled training class distribution:\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do a XGboost model\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "XGB_model= XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", n_estimators=200, max_depth=6, random_state=42)\n",
    "XGB_model.fit(X_train, y_train)\n",
    "xgb_y_pred = XGB_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, xgb_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do a XGboost model\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_smote_model= XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", n_estimators=200, max_depth=6, random_state=42)\n",
    "xgb_smote_model.fit(X_resampled_smote, y_resampled_smote)\n",
    "xgb_smote_y_pred = xgb_smote_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, xgb_smote_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do a XGboost model\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_smoteeen_model= XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", n_estimators=200, max_depth=6, random_state=42)\n",
    "xgb_smoteeen_model.fit(X_resampled_smoteenn, y_resampled_smoteenn)\n",
    "xgb_smoteeen_y_pred = xgb_smoteeen_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, xgb_smoteeen_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classification Report - Original Data\")\n",
    "print(classification_report(y_test, xgb_y_pred))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Resampled Data - SMOTE\")\n",
    "print(classification_report(y_test, xgb_smote_y_pred))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Resampled Data - SMOTEENN\")\n",
    "print(classification_report(y_test, xgb_smoteeen_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model_scaled = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model_scaled.fit(X_train_scaled, y_train)\n",
    "lr_y_pred_scaled = lr_model_scaled.predict(X_test_scaled)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, lr_y_pred_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model_unscaled = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model_unscaled.fit(X_train, y_train)\n",
    "lr_y_pred_unscaled = lr_model_unscaled.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, lr_y_pred_unscaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_smote_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_smote_model.fit(X_resampled_smote, y_resampled_smote)\n",
    "lr_smote_y_pred = lr_smote_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, lr_smote_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_smoteenn_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_smoteenn_model.fit(X_resampled_smoteenn, y_resampled_smoteenn)\n",
    "lr_smoteenn_y_pred = lr_smoteenn_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, lr_smoteenn_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classification Report - Original Data - Scaled\")\n",
    "print(classification_report(y_test, lr_y_pred_scaled))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Original Data - Unscaled\")\n",
    "print(classification_report(y_test, lr_y_pred_unscaled))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Resampled Data - SMOTE\")\n",
    "print(classification_report(y_test, lr_smote_y_pred))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Resampled Data - SMOTEENN\")\n",
    "print(classification_report(y_test, lr_smoteenn_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"LightGBM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"KNN Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"MLP (Neural Network) Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Most and Least Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importances_ from the trained RandomForest model\n",
    "feature_importances = .feature_importances_\n",
    "\n",
    "# Create a DataFrame that pairs each feature with its importance score\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order (most important at the top)\n",
    "importance_df.sort_values(by='importance', ascending=False, inplace=True)\n",
    "\n",
    "# Display the entire list\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
