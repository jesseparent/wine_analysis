{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from csv file\n",
    "red_wine_df = pd.read_csv('resources/winequality-red.csv', sep=';')\n",
    "white_wine_df = pd.read_csv('resources/winequality-white.csv', sep=';')\n",
    "\n",
    "# Create a new column 'color' and assign '1' to all rows for red wine\n",
    "red_wine_df['color'] = 1\n",
    "\n",
    "# Create a new column 'color' and assign '0' to all rows for white wine\n",
    "white_wine_df['color'] = 0\n",
    "\n",
    "# Create a new dataframe 'wine_df' by combining red_wine_df and white_wine_df and reset the index\n",
    "wine_df = pd.concat([red_wine_df, white_wine_df], ignore_index=True)\n",
    "\n",
    "# Display wine_df\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the dataframe\n",
    "wine_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe's info\n",
    "wine_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for null values\n",
    "wine_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any null values\n",
    "wine_df = wine_df.dropna().reset_index(drop=True)\n",
    "\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "wine_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index\n",
    "wine_df = wine_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a count of the unique values in the quality column\n",
    "wine_df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned data to a new csv file\n",
    "wine_df.to_csv('resources/winequality-cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Red Wine Scatter plot\n",
    "pd.plotting.scatter_matrix(red_wine_df, alpha=0.2, figsize=(20, 20), diagonal='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#White Wine Scatter plot\n",
    "pd.plotting.scatter_matrix(white_wine_df, alpha=0.2, figsize=(20, 20), diagonal='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(wine_df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red Wine Corelation\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(red_wine_df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# White Wine Corelation\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(white_wine_df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=wine_df, x='quality', hue='color')\n",
    "plt.title(\"Wine Quality Distribution by Type\")\n",
    "plt.xlabel(\"Quality\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Wine Color\", labels=[\"White\", \"Red\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the color column due to it not being relevant to quality, drop the free sulfur dioxide column due to it being highly correlated with total sulfur dioxide\n",
    "cleaned_wine = wine_df.drop(['color', 'free sulfur dioxide'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_wine2 = wine_df.drop(['color', 'free sulfur dioxide', 'chlorides', 'citric acid', 'fixed acidity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a value counts on the quality column\n",
    "cleaned_wine['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_wine2['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_plot = ['alcohol', 'volatile acidity', 'citric acid']\n",
    "for feature in features_to_plot:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='quality', y=feature, data=cleaned_wine)\n",
    "    plt.title(f\"{feature.title()} by Wine Quality\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['alcohol', 'volatile acidity', 'density', 'quality']\n",
    "sns.pairplot(cleaned_wine[top_features], hue='quality', palette='coolwarm')\n",
    "plt.suptitle(\"Pairplot of Selected Features\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='quality', y='alcohol', data=cleaned_wine)\n",
    "plt.title(\"Alcohol Distribution by Wine Quality\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Test and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for the quality column\n",
    "\n",
    "\n",
    "# Two Bins for 0-5 and 6-10\n",
    "bins = (0, 5, 10)\n",
    "\n",
    "# Name the bins 0 for low quality and 1 for high quality\n",
    "group_names = [0, 1]\n",
    "\n",
    "# Rename teh values in the quality column to the bin names\n",
    "cleaned_wine['quality'] = pd.cut(cleaned_wine['quality'], bins=bins, labels=group_names)\n",
    "\n",
    "# List unique values in the quality column\n",
    "cleaned_wine['quality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the values in the quality column to the bin names\n",
    "cleaned_wine2['quality'] = pd.cut(cleaned_wine2['quality'], bins=bins, labels=group_names)\n",
    "\n",
    "# List unique values in the quality column\n",
    "cleaned_wine2['quality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_wine['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_wine2['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = cleaned_wine.drop(columns= ['quality'])\n",
    "y= cleaned_wine['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target - cleaned 2\n",
    "X2 = cleaned_wine2.drop(columns= ['quality'])\n",
    "y2= cleaned_wine2['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets - cleaned 2\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size= 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned\n",
    "lr = sm.OLS(y_train, X_train).fit()\n",
    "pvals = lr.pvalues.sort_values()\n",
    "for index, value in pvals.items():\n",
    "    print(f\"{index}: {value:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned 2\n",
    "lr = sm.OLS(y_train2, X_train2).fit()\n",
    "pvals = lr.pvalues.sort_values()\n",
    "for index, value in pvals.items():\n",
    "    print(f\"{index}: {value:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnl = sm.MNLogit(y, X).fit()\n",
    "pvals = mnl.pvalues\n",
    "pvals_sorted = pvals.iloc[:, 0].sort_values()\n",
    "for index, value in pvals_sorted.items():\n",
    "    print(f\"{index}: {value:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = sm.Logit(y_train, X_train).fit()\n",
    "pvals = logit.pvalues.sort_values()\n",
    "for index, value in pvals.items():\n",
    "    print(f\"{index}: {value:4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features -  cleaned 2\n",
    "scaler2 = StandardScaler()\n",
    "X_train_scaled2 = pd.DataFrame(\n",
    "    scaler2.fit_transform(X_train2),\n",
    "    columns=X_train2.columns,\n",
    "    index=X_train2.index\n",
    ")\n",
    "\n",
    "X_test_scaled2 = pd.DataFrame(\n",
    "\n",
    "    scaler2.transform(X_test2),\n",
    "    columns=X_test2.columns,\n",
    "    index=X_test2.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = y_train.value_counts()\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts2 = y_train2.value_counts()\n",
    "print(class_counts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter   \n",
    "# Apply SMOTE on the scaled training data\n",
    "smote_model = SMOTE(random_state=42)\n",
    "X_resampled_smote, y_resampled_smote = smote_model.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Count distinct values for the resampled target data\n",
    "print(y_resampled_smote.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply SMOTE on the UNSCALED training data\n",
    "smote_model_unscaled = SMOTE(random_state=42)\n",
    "X_resampled_smote_unscaled, y_resampled_smote_unscaled = smote_model_unscaled.fit_resample(X_train, y_train)\n",
    "\n",
    "# Count distinct values for the resampled target data\n",
    "print(y_resampled_smote_unscaled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE on the scaled training data - cleaned 2\n",
    "smote_model2 = SMOTE(random_state=42)\n",
    "X_resampled_smote2, y_resampled_smote2 = smote_model2.fit_resample(X_train_scaled2, y_train2)\n",
    "\n",
    "# Count distinct values for the resampled target data\n",
    "print(y_resampled_smote2.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "# Apply SMOTEENN on the scaled training data\n",
    "smoteenn_model = SMOTEENN(random_state=42)\n",
    "X_resampled_smoteenn, y_resampled_smoteenn = smoteenn_model.fit_resample(X_train_scaled, y_train)\n",
    "# Count distinct values for the resampled target data\n",
    "print(y_resampled_smoteenn.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply SMOTEENN on the scaled training data - cleaned 2\n",
    "smoteenn_model2 = SMOTEENN(random_state=42)\n",
    "X_resampled_smoteenn2, y_resampled_smoteenn2 = smoteenn_model2.fit_resample(X_train_scaled2, y_train2)\n",
    "# Count distinct values for the resampled target data\n",
    "print(y_resampled_smoteenn2.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new RandomForestClassifier model - unscaled\n",
    "rf_model = RandomForestClassifier()\n",
    "# Fit the resampled data the new model\n",
    "rf_model.fit(X_train, y_train)\n",
    "# Predict labels for resampled testing features\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Print classification reports\n",
    "print(f\"Classification Report - Original Data\")\n",
    "print(classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new RandomForestClassifier model - unscaled, cleaned 2\n",
    "rf_model2 = RandomForestClassifier()\n",
    "# Fit the resampled data the new model\n",
    "rf_model2.fit(X_train2, y_train2)\n",
    "# Predict labels for resampled testing features\n",
    "rf_y_pred2 = rf_model2.predict(X_test2)\n",
    "\n",
    "# Print classification reports\n",
    "print(f\"Classification Report - Original Data, Cleaned 2\")\n",
    "print(classification_report(y_test2, rf_y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new RandomForestClassifier model - scaled\n",
    "rf_model_scaled = RandomForestClassifier()\n",
    "# Fit the resampled data the new model\n",
    "rf_model_scaled.fit(X_train_scaled, y_train)\n",
    "# Predict labels for resampled testing features\n",
    "rf_y_pred_scaled = rf_model_scaled.predict(X_test_scaled)\n",
    "\n",
    "# Print classification reports\n",
    "print(f\"Classification Report - Scaled Data\")\n",
    "print(classification_report(y_test, rf_y_pred_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new RandomForestClassifier model with SMOTE data - scaled\n",
    "rf_smote_model = RandomForestClassifier()\n",
    "# Fit the resampled data the new model\n",
    "rf_smote_model.fit(X_resampled_smote2, y_resampled_smote2)\n",
    "# Predict labels for resampled testing features\n",
    "rf_smote_y_pred = rf_smote_model.predict(X_test_scaled2)\n",
    "\n",
    "# Print classification reports\n",
    "print(f\"Classification Report - SMOTE Data\")\n",
    "print(classification_report(y_test2, rf_smote_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new RandomForestClassifier model with SMOTEENN data\n",
    "rf_smoteenn_model = RandomForestClassifier()\n",
    "# Fit the resampled data the new model\n",
    "rf_smoteenn_model.fit(X_resampled_smoteenn, y_resampled_smoteenn)\n",
    "# Predict labels for resampled testing features\n",
    "rf_smoteenn_y_pred = rf_smoteenn_model.predict(X_test_scaled)\n",
    "\n",
    "# Print classification reports\n",
    "print(f\"Classification Report - SMOTEENN Data\")\n",
    "print(classification_report(y_test, rf_smoteenn_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification reports\n",
    "print(f\"Classification Report - Original Data\")\n",
    "print(classification_report(y_test, rf_y_pred2))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Resampled Data - SMOTE\")\n",
    "print(classification_report(y_test, rf_smote_y_pred))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Resampled Data - SMOTEENN\")\n",
    "print(classification_report(y_test, rf_smoteenn_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the class distribution before SMOTE\n",
    "print(\"Original training class distribution:\", Counter(y_train))\n",
    "\n",
    "# Print the class distribution after SMOTE\n",
    "print(\"Resampled training class distribution:\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do a XGboost model\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "XGB_model= XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", n_estimators=200, max_depth=6, random_state=42)\n",
    "XGB_model.fit(X_train, y_train)\n",
    "xgb_y_pred = XGB_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, xgb_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do a XGboost model\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_smote_model= XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", n_estimators=200, max_depth=6, random_state=42)\n",
    "xgb_smote_model.fit(X_resampled_smote, y_resampled_smote)\n",
    "xgb_smote_y_pred = xgb_smote_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, xgb_smote_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do a XGboost model\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_smoteeen_model= XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", n_estimators=200, max_depth=6, random_state=42)\n",
    "xgb_smoteeen_model.fit(X_resampled_smoteenn, y_resampled_smoteenn)\n",
    "xgb_smoteeen_y_pred = xgb_smoteeen_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, xgb_smoteeen_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classification Report - Original Data\")\n",
    "print(classification_report(y_test, xgb_y_pred))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Resampled Data - SMOTE\")\n",
    "print(classification_report(y_test, xgb_smote_y_pred))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Resampled Data - SMOTEENN\")\n",
    "print(classification_report(y_test, xgb_smoteeen_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model_scaled = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model_scaled.fit(X_train_scaled, y_train)\n",
    "lr_y_pred_scaled = lr_model_scaled.predict(X_test_scaled)\n",
    "\n",
    "print(\"Classification Report: Scaled Data\")\n",
    "print(classification_report(y_test, lr_y_pred_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do a logistic regression model - cleaned 2\n",
    "\n",
    "lr_model_scaled2 = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model_scaled2.fit(X_train_scaled2, y_train2)\n",
    "lr_y_pred_scaled2 = lr_model_scaled2.predict(X_test_scaled2)\n",
    "\n",
    "print(\"Classification Report: Scaled Data, Cleaned 2\")\n",
    "print(classification_report(y_test2, lr_y_pred_scaled2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do a logistic regression model - unscaled\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model_unscaled = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model_unscaled.fit(X_train, y_train)\n",
    "lr_y_pred_unscaled = lr_model_unscaled.predict(X_test)\n",
    "\n",
    "print(\"Classification Report: Unscaled Data\")\n",
    "print(classification_report(y_test, lr_y_pred_unscaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do a logistic regression model - unscaled cleaned 2\n",
    "\n",
    "lr_model_unscaled2 = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model_unscaled2.fit(X_train2, y_train2)\n",
    "lr_y_pred_unscaled2 = lr_model_unscaled2.predict(X_test2)\n",
    "\n",
    "print(\"Classification Report: Unscaled Data, Cleaned 2\")\n",
    "print(classification_report(y_test2, lr_y_pred_unscaled2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do a logistic regression model - SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_smote_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_smote_model.fit(X_resampled_smote, y_resampled_smote)\n",
    "lr_smote_y_pred = lr_smote_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Classification Report: SMOTE Data\")\n",
    "print(classification_report(y_test, lr_smote_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do a logistic regression model - SMOTE cleaned 2\n",
    "\n",
    "lr_smote_model2 = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_smote_model2.fit(X_resampled_smote2, y_resampled_smote2)\n",
    "lr_smote_y_pred2 = lr_smote_model2.predict(X_test_scaled2)\n",
    "\n",
    "print(\"Classification Report: SMOTE Data, Cleaned 2\")\n",
    "print(classification_report(y_test2, lr_smote_y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do a logistic regression model - SMOTEENN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_smoteenn_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_smoteenn_model.fit(X_resampled_smoteenn, y_resampled_smoteenn)\n",
    "lr_smoteenn_y_pred = lr_smoteenn_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report: SMOTEENN Data\")\n",
    "print(classification_report(y_test, lr_smoteenn_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do a logistic regression model - SMOTEENN - cleaned 2\n",
    "\n",
    "lr_smoteenn_model2 = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_smoteenn_model2.fit(X_resampled_smoteenn2, y_resampled_smoteenn2)\n",
    "lr_smoteenn_y_pred2 = lr_smoteenn_model2.predict(X_test_scaled2)\n",
    "\n",
    "print(\"Classification Report: SMOTEENN Data, Cleaned 2\")\n",
    "print(classification_report(y_test2, lr_smoteenn_y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classification Report - Original Data - Scaled\")\n",
    "print(classification_report(y_test, lr_y_pred_scaled))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Original Data - Unscaled\")\n",
    "print(classification_report(y_test, lr_y_pred_unscaled))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Resampled Data - SMOTE\")\n",
    "print(classification_report(y_test, lr_smote_y_pred))\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Resampled Data - SMOTEENN\")\n",
    "print(classification_report(y_test, lr_smoteenn_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"LightGBM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"KNN Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"MLP (Neural Network) Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Most and Least Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importances_ from the trained RandomForest model\n",
    "rf_feature_importances = rf_model2.feature_importances_\n",
    "\n",
    "# Create a DataFrame that pairs each feature with its importance score\n",
    "rf_importance_df = pd.DataFrame({\n",
    "    'feature': X2.columns,\n",
    "    'importance': rf_feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order (most important at the top)\n",
    "rf_importance_df.sort_values(by='importance', ascending=False, inplace=True)\n",
    "\n",
    "# Display the entire list\n",
    "print(rf_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],  # Regularization type\n",
    "    'solver': ['liblinear', 'saga']  # Solvers that support L1/L2 penalties\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=500, random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit to training data\n",
    "grid_search.fit(X_train_scaled2, y_train2)\n",
    "\n",
    "# Best parameters & score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test data\n",
    "best_lr = grid_search.best_estimator_\n",
    "y_pred_best = best_lr.predict(X_test_scaled2)\n",
    "print(classification_report(y_test2, y_pred_best))\n",
    "print(f\"Accuracy: {accuracy_score(y_test2, y_pred_best):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define hyperparameter space\n",
    "param_dist = {\n",
    "    'C': np.logspace(-3, 3, 10),  # Wide range of C values\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    LogisticRegression(max_iter=500, random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,  # Number of random samples\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "random_search.fit(X_train_scaled2, y_train2)\n",
    "\n",
    "# Best results\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Best Accuracy:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate on test data\n",
    "best_lr_random = random_search.best_estimator_\n",
    "y_pred_random = best_lr_random.predict(X_test_scaled2)\n",
    "print(classification_report(y_test2, y_pred_random))\n",
    "print(f\"Accuracy: {accuracy_score(y_test2, y_pred_random):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a manual For Loop, instead of GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "C_values = [0.01, 0.1, 1, 10, 100]  # Regularization strength\n",
    "penalties = ['l1', 'l2']  # Regularization types\n",
    "solvers = ['liblinear', 'saga']  # Solvers that support L1/L2 penalties\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "best_y_pred = None\n",
    "\n",
    "# Nested loops for hyperparameter tuning\n",
    "for C in C_values:\n",
    "    for penalty in penalties:\n",
    "        for solver in solvers:\n",
    "            try:\n",
    "                # Initialize model with hyperparameters\n",
    "                model = LogisticRegression(C=C, penalty=penalty, solver=solver, max_iter=500, random_state=42)\n",
    "\n",
    "                # Train the model\n",
    "                model.fit(X_train_scaled2, y_train2)\n",
    "\n",
    "                # Predict on validation set\n",
    "                y_pred = model.predict(X_test_scaled2)\n",
    "\n",
    "                # Compute accuracy\n",
    "                accuracy = accuracy_score(y_test2, y_pred)\n",
    "\n",
    "                # Print results\n",
    "                print(f\"C={C}, penalty={penalty}, solver={solver} -> Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "                # Track the best model\n",
    "                if accuracy > best_score:\n",
    "                    best_score = accuracy\n",
    "                    best_params = {'C': C, 'penalty': penalty, 'solver': solver}\n",
    "                    best_y_pred = y_pred\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping C={C}, penalty={penalty}, solver={solver} due to error: {e}\")\n",
    "\n",
    "# Print the best hyperparameters and accuracy\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_score)\n",
    "print(classification_report(y_test2, best_y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test2, best_y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV with RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit on training data\n",
    "grid_search.fit(X_train2, y_train2)\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(\"Best Parameters from GridSearchCV:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test data\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_best = best_rf.predict(X_test2)\n",
    "print(classification_report(y_test2, y_pred_best))\n",
    "print(f\"Accuracy: {accuracy_score(y_test2, y_pred_best):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV with RandomForest\n",
    "\n",
    "# Define hyperparameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 300, 50),\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Instantiate RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_distributions=param_dist,\n",
    "                                   n_iter=20, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "# Fit on training data\n",
    "random_search.fit(X_train2, y_train2)\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(\"Best Parameters from RandomizedSearchCV:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate on test data\n",
    "best_rf_random = random_search.best_estimator_\n",
    "y_pred_random = best_rf_random.predict(X_test2)\n",
    "print(classification_report(y_test2, y_pred_random))\n",
    "print(f\"Accuracy: {accuracy_score(y_test2, y_pred_random):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV with RandomForest - TEST\n",
    "\n",
    "# Define hyperparameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 300, 50),\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Instantiate RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_distributions=param_dist,\n",
    "                                   n_iter=20, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "# Fit on training data\n",
    "random_search.fit(X_resampled_smote2, y_resampled_smote2)\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(\"Best Parameters from RandomizedSearchCV:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate on test data\n",
    "best_rf_random = random_search.best_estimator_\n",
    "y_pred_random = best_rf_random.predict(X_test_scaled2)\n",
    "print(classification_report(y_test2, y_pred_random))\n",
    "print(f\"Accuracy: {accuracy_score(y_test2, y_pred_random):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested For Loop with RandomForest\n",
    "\n",
    "# Define hyperparameter values to iterate over\n",
    "n_estimators_list = range(1, 101)    \n",
    "max_depth_list = [None, 10, 20]\n",
    "min_samples_split_list = [2, 5, 10]\n",
    "min_samples_leaf_list = [1, 2, 4]\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "best_y_pred = None\n",
    "\n",
    "# Nested loops for hyperparameter tuning\n",
    "for n_estimators in n_estimators_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        for min_samples_split in min_samples_split_list:\n",
    "            for min_samples_leaf in min_samples_leaf_list:\n",
    "                # Initialize model\n",
    "                rf_model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                                  min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                                  random_state=42)\n",
    "                \n",
    "                # Fit model\n",
    "                rf_model.fit(X_train2, y_train2)\n",
    "                \n",
    "                # Predict on test data\n",
    "                y_pred = rf_model.predict(X_test2)\n",
    "                \n",
    "                # Compute accuracy\n",
    "                accuracy = accuracy_score(y_test2, y_pred)\n",
    "                \n",
    "                # Track best model\n",
    "                if accuracy > best_score:\n",
    "                    best_score = accuracy\n",
    "                    best_params = {\n",
    "                        'n_estimators': n_estimators,\n",
    "                        'max_depth': max_depth,\n",
    "                        'min_samples_split': min_samples_split,\n",
    "                        'min_samples_leaf': min_samples_leaf\n",
    "                    }\n",
    "                    best_y_pred = y_pred\n",
    "\n",
    "                print(f\"n_estimators={n_estimators}, max_depth={max_depth}, min_samples_split={min_samples_split}, min_samples_leaf={min_samples_leaf} -> Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"\\nBest Hyperparameters from Nested Loops:\", best_params)\n",
    "print(\"Best Accuracy:\", best_score)\n",
    "print(classification_report(y_test2, best_y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test2, best_y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC RUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# calculate  a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "#print the results of the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AUC ROC curve tools\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC Model vizualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a ROC curve\n",
    "rf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test2, rf_y_pred2)\n",
    "# calculate the ROC AUC\n",
    "rf_roc_auc = roc_auc_score(y_test2, rf_y_pred2)\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rf_fpr, rf_tpr, color='orange', label='ROC curve (area = %0.2f)' % roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get predicted probabilities for the positive class, not predicted labels\n",
    "rf_y_proba = rf_model2.predict_proba(X_test2)[:, 1]\n",
    "\n",
    "#Calculate the ROC curve using predicted probabilities\n",
    "rf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test2, rf_y_proba)\n",
    "rf_roc_auc = roc_auc_score(y_test2, rf_y_proba)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rf_fpr, rf_tpr, color='orange', \n",
    "         label='ROC curve (AUC = %0.2f)' % rf_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--', label='Random guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Using the same rf model above\n",
    "rf_result = permutation_importance(rf_model2, X_test2, y_test2, n_repeats=10, random_state=42)\n",
    "\n",
    "rf_importances = rf_result.importances_mean\n",
    "rf_feature_names = X_test2.columns\n",
    "rf_perm_imp_df = pd.DataFrame({\n",
    "    'Feature': rf_feature_names,\n",
    "    'Importance': rf_importances\n",
    "})\n",
    "rf_perm_imp_df.sort_values('Importance', ascending=True, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(rf_perm_imp_df['Feature'], rf_perm_imp_df['Importance'])\n",
    "plt.title(\"Permutation Importance\")\n",
    "plt.xlabel(\"Mean Decrease in Accuracy\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR model visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a ROC curve\n",
    "lr_fpr, lr_tpr, lr_thresholds = roc_curve(y_test2, lr_y_pred_scaled2)\n",
    "# calculate the ROC AUC\n",
    "lr_roc_auc = roc_auc_score(y_test2, lr_y_pred_scaled2)\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lr_fpr, lr_tpr, color='orange', label='ROC curve (area = %0.2f)' % roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get predicted probabilities for the positive class, not predicted labels\n",
    "lr_y_proba = lr_model_scaled2.predict_proba(X_test2)[:, 1]\n",
    "\n",
    "#Calculate the ROC curve using predicted probabilities\n",
    "lr_fpr, lr_tpr, thresholds = roc_curve(y_test2, lr_y_proba)\n",
    "lr_roc_auc = roc_auc_score(y_test2, lr_y_proba)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lr_fpr, lr_tpr, color='orange', \n",
    "         label='ROC curve (AUC = %0.2f)' % lr_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--', label='Random guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Using the same rf model above\n",
    "lr_result = permutation_importance(lr_model_scaled2, X_test2, y_test2, n_repeats=10, random_state=42)\n",
    "\n",
    "lr_importances = lr_result.importances_mean\n",
    "lr_feature_names = X_test2.columns\n",
    "lr_perm_imp_df = pd.DataFrame({\n",
    "    'Feature': lr_feature_names,\n",
    "    'Importance': lr_importances\n",
    "})\n",
    "lr_perm_imp_df.sort_values('Importance', ascending=True, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(lr_perm_imp_df['Feature'], lr_perm_imp_df['Importance'])\n",
    "plt.title(\"Permutation Importance\")\n",
    "plt.xlabel(\"Mean Decrease in Accuracy\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
